{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Basics with Omni-Dev Agent\n",
    "\n",
    "This notebook demonstrates the core computer vision capabilities of Omni-Dev Agent, including:\n",
    "\n",
    "- Object Detection with YOLOv8\n",
    "- OCR (Optical Character Recognition)\n",
    "- Face Recognition\n",
    "- Image Classification\n",
    "- Real-time Processing\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the Omni-Dev Agent server running:\n",
    "\n",
    "```bash\n",
    "cd src\n",
    "python main.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import base64\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"http://localhost:5000\"\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    \"\"\"Convert image file to base64 string.\"\"\"\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_data = base64.b64encode(f.read()).decode()\n",
    "    return f\"data:image/jpeg;base64,{image_data}\"\n",
    "\n",
    "def numpy_to_base64(image_array):\n",
    "    \"\"\"Convert numpy array to base64 string.\"\"\"\n",
    "    _, buffer = cv2.imencode('.jpg', image_array)\n",
    "    image_base64 = base64.b64encode(buffer).decode()\n",
    "    return f\"data:image/jpeg;base64,{image_base64}\"\n",
    "\n",
    "def display_detection_results(image_path, detections):\n",
    "    \"\"\"Display image with detection bounding boxes.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    for detection in detections:\n",
    "        bbox = detection['bbox']\n",
    "        x1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Add label\n",
    "        label = f\"{detection['class_name']}: {detection['confidence']:.2f}\"\n",
    "        cv2.putText(image, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Object Detection Results ({len(detections)} objects found)\")\n",
    "    plt.show()\n",
    "\n",
    "def test_server_connection():\n",
    "    \"\"\"Test if the Omni-Dev Agent server is running.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{BASE_URL}/\")\n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Server is running!\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Server returned status code: {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Server connection failed: {e}\")\n",
    "        print(\"Make sure the server is running: cd src && python main.py\")\n",
    "        return False\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_server_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Object Detection with YOLOv8\n",
    "\n",
    "Let's start with object detection using the YOLOv8 model. We'll demonstrate detection on various types of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample image for testing (you can replace with your own image)\n",
    "# Let's create a simple test image with geometric shapes\n",
    "test_image = np.zeros((400, 600, 3), dtype=np.uint8)\n",
    "\n",
    "# Add some colored rectangles and circles\n",
    "cv2.rectangle(test_image, (50, 50), (200, 150), (0, 255, 0), -1)  # Green rectangle\n",
    "cv2.circle(test_image, (400, 200), 80, (0, 0, 255), -1)  # Red circle\n",
    "cv2.rectangle(test_image, (250, 250), (450, 350), (255, 0, 0), -1)  # Blue rectangle\n",
    "\n",
    "# Save the test image\n",
    "cv2.imwrite('test_image.jpg', test_image)\n",
    "\n",
    "# Display the test image\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image for Object Detection\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Test image created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection\n",
    "def detect_objects(image_path, confidence_threshold=0.25):\n",
    "    \"\"\"Perform object detection using the API.\"\"\"\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/vision/detect\",\n",
    "        json={\n",
    "            'image': image_base64,\n",
    "            'confidence': confidence_threshold\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Test object detection\n",
    "detection_result = detect_objects('test_image.jpg', confidence_threshold=0.1)\n",
    "\n",
    "if detection_result:\n",
    "    print(f\"\\nüìä Object Detection Results:\")\n",
    "    print(f\"Timestamp: {detection_result['timestamp']}\")\n",
    "    print(f\"Objects found: {detection_result['count']}\")\n",
    "    \n",
    "    for i, detection in enumerate(detection_result['detections'], 1):\n",
    "        print(f\"\\n{i}. {detection['class_name']}\")\n",
    "        print(f\"   Confidence: {detection['confidence']:.3f}\")\n",
    "        print(f\"   Bounding Box: {detection['bbox']}\")\n",
    "    \n",
    "    # Display results with bounding boxes\n",
    "    if detection_result['detections']:\n",
    "        display_detection_results('test_image.jpg', detection_result['detections'])\n",
    "else:\n",
    "    print(\"Object detection failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OCR (Optical Character Recognition)\n",
    "\n",
    "Let's test text extraction from images using OCR capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a test image with text\n",
    "text_image = np.ones((200, 600, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "# Add text to the image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(text_image, 'Hello, Omni-Dev Agent!', (50, 80), font, 1, (0, 0, 0), 2)\n",
    "cv2.putText(text_image, 'OCR Test - 123456', (50, 140), font, 1, (0, 0, 0), 2)\n",
    "\n",
    "# Save the text image\n",
    "cv2.imwrite('text_image.jpg', text_image)\n",
    "\n",
    "# Display the text image\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.imshow(cv2.cvtColor(text_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image for OCR\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Text image created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform OCR\n",
    "def extract_text(image_path, language='eng'):\n",
    "    \"\"\"Extract text from image using OCR API.\"\"\"\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/vision/ocr\",\n",
    "        json={\n",
    "            'image': image_base64,\n",
    "            'language': language\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Test OCR\n",
    "ocr_result = extract_text('text_image.jpg')\n",
    "\n",
    "if ocr_result:\n",
    "    print(f\"\\nüìù OCR Results:\")\n",
    "    print(f\"Timestamp: {ocr_result['timestamp']}\")\n",
    "    print(f\"Language: {ocr_result['language']}\")\n",
    "    print(f\"Confidence: {ocr_result['confidence']:.3f}\")\n",
    "    print(f\"\\nExtracted Text:\")\n",
    "    print(f'\"{ocr_result[\"text\"]}\"')\n",
    "else:\n",
    "    print(\"OCR failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Face Recognition\n",
    "\n",
    "Let's test the face recognition capabilities. Note that this requires pre-trained face encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple face-like image (or use a real photo)\n",
    "# For demonstration, we'll create a simple circular \"face\"\n",
    "face_image = np.ones((300, 300, 3), dtype=np.uint8) * 240  # Light gray background\n",
    "\n",
    "# Draw a simple \"face\"\n",
    "cv2.circle(face_image, (150, 150), 80, (255, 220, 177), -1)  # Face circle\n",
    "cv2.circle(face_image, (130, 130), 8, (0, 0, 0), -1)  # Left eye\n",
    "cv2.circle(face_image, (170, 130), 8, (0, 0, 0), -1)  # Right eye\n",
    "cv2.ellipse(face_image, (150, 170), (20, 10), 0, 0, 180, (0, 0, 0), 2)  # Smile\n",
    "\n",
    "# Save the face image\n",
    "cv2.imwrite('face_image.jpg', face_image)\n",
    "\n",
    "# Display the face image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Test Image for Face Recognition\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"Face image created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform face recognition\n",
    "def recognize_faces(image_path):\n",
    "    \"\"\"Recognize faces in image using the API.\"\"\"\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/vision/face/identify\",\n",
    "        json={\n",
    "            'image': image_base64\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Test face recognition\n",
    "face_result = recognize_faces('face_image.jpg')\n",
    "\n",
    "if face_result:\n",
    "    print(f\"\\nüë§ Face Recognition Results:\")\n",
    "    print(f\"Timestamp: {face_result['timestamp']}\")\n",
    "    print(f\"Faces found: {face_result['count']}\")\n",
    "    \n",
    "    for i, face in enumerate(face_result['faces'], 1):\n",
    "        print(f\"\\n{i}. Face ID: {face['face_id']}\")\n",
    "        print(f\"   Name: {face['name']}\")\n",
    "        print(f\"   Confidence: {face['confidence']:.3f}\")\n",
    "        print(f\"   Bounding Box: {face['bbox']}\")\n",
    "else:\n",
    "    print(\"Face recognition failed or no faces detected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Classification\n",
    "\n",
    "Let's test image classification to categorize entire images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform image classification\n",
    "def classify_image(image_path, top_k=5):\n",
    "    \"\"\"Classify image using the API.\"\"\"\n",
    "    image_base64 = encode_image_to_base64(image_path)\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/vision/classify\",\n",
    "        json={\n",
    "            'image': image_base64,\n",
    "            'top_k': top_k\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "# Test image classification\n",
    "classification_result = classify_image('test_image.jpg', top_k=3)\n",
    "\n",
    "if classification_result:\n",
    "    print(f\"\\nüè∑Ô∏è Image Classification Results:\")\n",
    "    print(f\"Timestamp: {classification_result['timestamp']}\")\n",
    "    print(f\"Processing Time: {classification_result['processing_time']:.3f}s\")\n",
    "    print(f\"\\nTop Predictions:\")\n",
    "    \n",
    "    for i, prediction in enumerate(classification_result['classifications'], 1):\n",
    "        print(f\"{i}. {prediction['class_name']}\")\n",
    "        print(f\"   Confidence: {prediction['confidence']:.3f}\")\n",
    "        print(f\"   Class ID: {prediction['class_id']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"Image classification failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing Example\n",
    "\n",
    "Let's demonstrate processing multiple images in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple test images\n",
    "test_images = []\n",
    "\n",
    "for i in range(3):\n",
    "    # Create different colored images\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]  # Red, Green, Blue\n",
    "    img = np.zeros((200, 200, 3), dtype=np.uint8)\n",
    "    cv2.rectangle(img, (50, 50), (150, 150), colors[i], -1)\n",
    "    \n",
    "    filename = f'batch_image_{i+1}.jpg'\n",
    "    cv2.imwrite(filename, img)\n",
    "    test_images.append(filename)\n",
    "\n",
    "print(f\"Created {len(test_images)} test images for batch processing\")\n",
    "\n",
    "# Display the batch images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Image {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images in batch\n",
    "def process_batch_images(image_paths):\n",
    "    \"\"\"Process multiple images for object detection.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"Processing image {i+1}/{len(image_paths)}: {image_path}\")\n",
    "        result = detect_objects(image_path, confidence_threshold=0.1)\n",
    "        results.append({\n",
    "            'image_path': image_path,\n",
    "            'result': result\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process batch\n",
    "batch_results = process_batch_images(test_images)\n",
    "\n",
    "print(f\"\\nüìä Batch Processing Summary:\")\n",
    "print(f\"Total images processed: {len(batch_results)}\")\n",
    "\n",
    "for i, batch_result in enumerate(batch_results):\n",
    "    result = batch_result['result']\n",
    "    if result:\n",
    "        print(f\"\\nImage {i+1}: {batch_result['image_path']}\")\n",
    "        print(f\"  Objects detected: {result['count']}\")\n",
    "        for detection in result['detections']:\n",
    "            print(f\"    - {detection['class_name']}: {detection['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nImage {i+1}: Processing failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis\n",
    "\n",
    "Let's analyze the performance of different vision tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_vision_apis(image_path, num_iterations=5):\n",
    "    \"\"\"Benchmark different vision APIs.\"\"\"\n",
    "    apis = {\n",
    "        'Object Detection': lambda: detect_objects(image_path),\n",
    "        'OCR': lambda: extract_text(image_path),\n",
    "        'Face Recognition': lambda: recognize_faces(image_path),\n",
    "        'Image Classification': lambda: classify_image(image_path)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for api_name, api_func in apis.items():\n",
    "        times = []\n",
    "        success_count = 0\n",
    "        \n",
    "        print(f\"\\nBenchmarking {api_name}...\")\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            start_time = time.time()\n",
    "            result = api_func()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            processing_time = end_time - start_time\n",
    "            times.append(processing_time)\n",
    "            \n",
    "            if result is not None:\n",
    "                success_count += 1\n",
    "            \n",
    "            print(f\"  Iteration {i+1}: {processing_time:.3f}s\")\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        std_time = np.std(times)\n",
    "        success_rate = success_count / num_iterations\n",
    "        \n",
    "        results[api_name] = {\n",
    "            'avg_time': avg_time,\n",
    "            'std_time': std_time,\n",
    "            'success_rate': success_rate,\n",
    "            'times': times\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"üöÄ Starting Performance Benchmark...\")\n",
    "benchmark_results = benchmark_vision_apis('test_image.jpg', num_iterations=3)\n",
    "\n",
    "print(f\"\\nüìà Performance Benchmark Results:\")\n",
    "print(f\"{'API':<20} {'Avg Time (s)':<12} {'Std Dev':<10} {'Success Rate':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for api_name, stats in benchmark_results.items():\n",
    "    print(f\"{api_name:<20} {stats['avg_time']:<12.3f} {stats['std_time']:<10.3f} {stats['success_rate']:<12.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "api_names = list(benchmark_results.keys())\n",
    "avg_times = [stats['avg_time'] for stats in benchmark_results.values()]\n",
    "std_times = [stats['std_time'] for stats in benchmark_results.values()]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create bar plot with error bars\n",
    "bars = plt.bar(api_names, avg_times, yerr=std_times, capsize=5, alpha=0.7)\n",
    "plt.xlabel('Vision API')\n",
    "plt.ylabel('Processing Time (seconds)')\n",
    "plt.title('Vision API Performance Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, avg_time in zip(bars, avg_times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{avg_time:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Advanced Integration Example\n",
    "\n",
    "Let's create a comprehensive example that combines multiple vision capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_vision_analysis(image_path):\n",
    "    \"\"\"Perform comprehensive vision analysis on an image.\"\"\"\n",
    "    print(f\"üîç Performing comprehensive analysis on: {image_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load and display the image\n",
    "    img = cv2.imread(image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.title(f\"Input Image: {image_path}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    analysis_results = {}\n",
    "    \n",
    "    # 1. Object Detection\n",
    "    print(\"\\n1Ô∏è‚É£ Running Object Detection...\")\n",
    "    detection_result = detect_objects(image_path)\n",
    "    if detection_result:\n",
    "        analysis_results['object_detection'] = detection_result\n",
    "        print(f\"   ‚úÖ Found {detection_result['count']} objects\")\n",
    "        for det in detection_result['detections']:\n",
    "            print(f\"      - {det['class_name']}: {det['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Object detection failed\")\n",
    "    \n",
    "    # 2. OCR\n",
    "    print(\"\\n2Ô∏è‚É£ Running OCR...\")\n",
    "    ocr_result = extract_text(image_path)\n",
    "    if ocr_result and ocr_result['text'].strip():\n",
    "        analysis_results['ocr'] = ocr_result\n",
    "        print(f\"   ‚úÖ Extracted text (confidence: {ocr_result['confidence']:.3f})\")\n",
    "        print(f\"      '{ocr_result['text'].strip()}'\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No text detected\")\n",
    "    \n",
    "    # 3. Face Recognition\n",
    "    print(\"\\n3Ô∏è‚É£ Running Face Recognition...\")\n",
    "    face_result = recognize_faces(image_path)\n",
    "    if face_result and face_result['count'] > 0:\n",
    "        analysis_results['face_recognition'] = face_result\n",
    "        print(f\"   ‚úÖ Found {face_result['count']} faces\")\n",
    "        for face in face_result['faces']:\n",
    "            print(f\"      - {face['name']}: {face['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No faces detected\")\n",
    "    \n",
    "    # 4. Image Classification\n",
    "    print(\"\\n4Ô∏è‚É£ Running Image Classification...\")\n",
    "    classification_result = classify_image(image_path, top_k=3)\n",
    "    if classification_result:\n",
    "        analysis_results['classification'] = classification_result\n",
    "        print(f\"   ‚úÖ Top classifications:\")\n",
    "        for cls in classification_result['classifications']:\n",
    "            print(f\"      - {cls['class_name']}: {cls['confidence']:.3f}\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Image classification failed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ú® Comprehensive analysis complete!\")\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "# Run comprehensive analysis on our test image\n",
    "comprehensive_results = comprehensive_vision_analysis('test_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Handling and Best Practices\n",
    "\n",
    "Let's demonstrate proper error handling and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_vision_api_call(api_endpoint, payload, max_retries=3, retry_delay=1):\n",
    "    \"\"\"Make robust API calls with error handling and retries.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{BASE_URL}{api_endpoint}\",\n",
    "                json=payload,\n",
    "                timeout=30  # 30 second timeout\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 429:  # Rate limited\n",
    "                print(f\"Rate limited. Waiting {retry_delay}s before retry...\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"API returned status {response.status_code}: {response.text}\")\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Request timed out. Attempt {attempt + 1}/{max_retries}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "                \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"Connection error. Attempt {attempt + 1}/{max_retries}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"Failed after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# Test robust API call\n",
    "print(\"Testing robust API call with error handling...\")\n",
    "\n",
    "test_payload = {\n",
    "    'image': encode_image_to_base64('test_image.jpg'),\n",
    "    'confidence': 0.25\n",
    "}\n",
    "\n",
    "robust_result = robust_vision_api_call('/vision/detect', test_payload)\n",
    "\n",
    "if robust_result:\n",
    "    print(f\"‚úÖ Robust API call successful: {robust_result['count']} objects detected\")\n",
    "else:\n",
    "    print(\"‚ùå Robust API call failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Optimization Tips\n",
    "\n",
    "Let's demonstrate some performance optimization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_image_for_api(image_path, max_size=800, quality=85):\n",
    "    \"\"\"Optimize image size and quality for API calls.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Get original dimensions\n",
    "    h, w = img.shape[:2]\n",
    "    original_size = len(cv2.imencode('.jpg', img)[1])\n",
    "    \n",
    "    # Resize if too large\n",
    "    if max(h, w) > max_size:\n",
    "        if w > h:\n",
    "            new_w = max_size\n",
    "            new_h = int(h * max_size / w)\n",
    "        else:\n",
    "            new_h = max_size\n",
    "            new_w = int(w * max_size / h)\n",
    "        \n",
    "        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # Compress with specified quality\n",
    "    encode_params = [cv2.IMWRITE_JPEG_QUALITY, quality]\n",
    "    _, buffer = cv2.imencode('.jpg', img, encode_params)\n",
    "    optimized_size = len(buffer)\n",
    "    \n",
    "    # Create base64\n",
    "    image_base64 = base64.b64encode(buffer).decode()\n",
    "    data_url = f\"data:image/jpeg;base64,{image_base64}\"\n",
    "    \n",
    "    print(f\"Image optimization results:\")\n",
    "    print(f\"  Original size: {original_size:,} bytes\")\n",
    "    print(f\"  Optimized size: {optimized_size:,} bytes\")\n",
    "    print(f\"  Size reduction: {(1 - optimized_size/original_size)*100:.1f}%\")\n",
    "    print(f\"  Dimensions: {img.shape[1]}x{img.shape[0]}\")\n",
    "    \n",
    "    return data_url\n",
    "\n",
    "# Test image optimization\n",
    "print(\"üîß Testing image optimization...\")\n",
    "optimized_image = optimize_image_for_api('test_image.jpg')\n",
    "\n",
    "# Test API call with optimized image\n",
    "optimized_result = requests.post(\n",
    "    f\"{BASE_URL}/vision/detect\",\n",
    "    json={\n",
    "        'image': optimized_image,\n",
    "        'confidence': 0.25\n",
    "    }\n",
    ")\n",
    "\n",
    "if optimized_result.status_code == 200:\n",
    "    result_data = optimized_result.json()\n",
    "    print(f\"‚úÖ Optimized image processing successful: {result_data['count']} objects detected\")\n",
    "else:\n",
    "    print(f\"‚ùå Optimized image processing failed: {optimized_result.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "\n",
    "Let's clean up the temporary files we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List of files to clean up\n",
    "temp_files = [\n",
    "    'test_image.jpg',\n",
    "    'text_image.jpg',\n",
    "    'face_image.jpg',\n",
    "    'batch_image_1.jpg',\n",
    "    'batch_image_2.jpg',\n",
    "    'batch_image_3.jpg'\n",
    "]\n",
    "\n",
    "print(\"üßπ Cleaning up temporary files...\")\n",
    "\n",
    "for file in temp_files:\n",
    "    try:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "            print(f\"  ‚úÖ Removed {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Failed to remove {file}: {e}\")\n",
    "\n",
    "print(\"\\n‚ú® Cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've demonstrated:\n",
    "\n",
    "1. **Object Detection** - Detecting and localizing objects in images using YOLOv8\n",
    "2. **OCR** - Extracting text from images with confidence scoring\n",
    "3. **Face Recognition** - Identifying faces with bounding boxes\n",
    "4. **Image Classification** - Categorizing entire images with top-k predictions\n",
    "5. **Batch Processing** - Processing multiple images efficiently\n",
    "6. **Performance Analysis** - Benchmarking different APIs\n",
    "7. **Comprehensive Analysis** - Combining multiple vision capabilities\n",
    "8. **Error Handling** - Robust API calls with retries\n",
    "9. **Optimization** - Image optimization techniques for better performance\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- The Omni-Dev Agent provides a unified API for multiple computer vision tasks\n",
    "- All APIs return consistent JSON responses with timestamps and confidence scores\n",
    "- Images should be base64-encoded with data URL prefixes\n",
    "- Error handling and retries are important for production use\n",
    "- Image optimization can significantly improve processing speed\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore real-time processing with WebSocket streaming\n",
    "- Integrate with camera systems for live video analysis\n",
    "- Implement custom vision pipelines\n",
    "- Deploy the system in production environments\n",
    "\n",
    "For more advanced examples and integration patterns, check out the other notebooks in this series!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
